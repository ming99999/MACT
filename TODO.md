# MACT LangGraph - 향후 작업 목록

## 현재 상태
- **Phase 1-2 완료**: 기본 구현 완성 ✅
- **기능 검증**: 모든 핵심 기능 동작 확인 ✅
- **문서화**: 포괄적인 문서 작성 완료 ✅

## 즉시 가능한 작업

### 1. 실행 및 테스트 🚀 (우선순위: 높음)
- [ ] **환경 설정**
  - `.env` 파일 생성 및 API 키 설정
  - 의존성 패키지 설치 (`pip install -r requirements.txt`)

- [ ] **기본 실행 테스트**
  - 예제 스크립트 실행: `python run_examples.py`
  - 단위 테스트 실행: `pytest tests/test_basic.py`

- [ ] **MMQA 데이터 실행**
  - MMQA 샘플 데이터로 실행: `python main.py --dataset_path ../datasets_examples/mmqa_samples.json --debug`
  - 결과 검증 및 성능 측정

### 2. 성능 벤치마킹 📊 (우선순위: 중간)
- [ ] **정확도 평가**
  - MMQA 전체 데이터셋에서 실행
  - 원본 MACT와 성능 비교
  - 다양한 보상 함수별 성능 비교

- [ ] **실행 시간 측정**
  - 질문당 평균 처리 시간
  - 메모리 사용량 모니터링
  - 토큰 사용량 추적

### 3. 설정 최적화 ⚙️ (우선순위: 중간)
- [ ] **하이퍼파라미터 튜닝**
  - `plan_sample`, `code_sample` 최적값 찾기
  - `max_steps` 적정값 결정
  - 모델별 최적 설정 발견

- [ ] **프롬프트 개선**
  - MMQA 특화 프롬프트 최적화
  - Few-shot 예제 개선
  - 오류 복구 프롬프트 추가

## 단기 개선 작업 (1-2주)

### 4. 고급 보상 함수 구현 🎯 (우선순위: 중간)
- [ ] **LogP 보상 함수 정교화**
  - 실제 로그 확률 계산 구현
  - 모델별 확률 정규화
  - 확률 기반 선택 로직 개선

- [ ] **Rollout 보상 함수 구현**
  - 미래 상태 시뮬레이션
  - 다단계 평가 로직
  - 효율적인 롤아웃 알고리즘

### 5. 오류 처리 강화 🛡️ (우선순위: 높음)
- [ ] **복구 메커니즘 개선**
  - 코드 실행 실패 시 재시도 로직
  - 대안 액션 제안 기능
  - 부분 실패 상황 처리

- [ ] **검증 로직 추가**
  - 생성된 코드 안전성 검증
  - 무한 루프 방지
  - 메모리 사용량 제한

### 6. 모니터링 및 디버깅 🔍 (우선순위: 중간)
- [ ] **실행 추적 개선**
  - 더 상세한 로깅
  - 단계별 중간 결과 저장
  - 시각화 도구 추가

- [ ] **디버깅 도구**
  - 그래프 실행 시각화
  - 상태 변화 추적
  - 성능 프로파일링

## 중기 확장 작업 (2-4주)

### 7. 추가 데이터셋 지원 📈 (우선순위: 중간)
- [ ] **WTQ 데이터셋 통합**
  - WTQ 특화 프롬프트 작성
  - 평가 메트릭 추가
  - 성능 벤치마킹

- [ ] **TAT-QA 데이터셋 지원**
  - 하이브리드 추론 로직 추가
  - 숫자/기호 추론 강화
  - TAT 평가 스크립트 통합

- [ ] **CRT, SciTab 지원**
  - 데이터셋별 특화 처리
  - 도메인 특화 도구 추가
  - 통합 평가 프레임워크

### 8. 성능 최적화 ⚡ (우선순위: 중간)
- [ ] **병렬 처리 구현**
  - 배치 단위 비동기 처리
  - 동시 실행 제한 관리
  - 리소스 효율성 개선

- [ ] **캐싱 시스템**
  - LLM 응답 캐싱
  - 중간 결과 캐싱
  - 스마트 캐시 무효화

- [ ] **메모리 최적화**
  - 대용량 테이블 처리 개선
  - 메모리 사용량 제한
  - 가비지 컬렉션 최적화

### 9. 고급 기능 추가 🌟 (우선순위: 낮음)
- [ ] **적응형 샘플링**
  - 동적 `plan_sample` 조절
  - 성공률 기반 샘플 수 조정
  - 복잡도 기반 샘플링

- [ ] **학습 기반 개선**
  - 성공 패턴 학습
  - 실패 패턴 회피
  - 동적 프롬프트 조정

## 장기 발전 작업 (1-2개월)

### 10. 프로덕션 배포 🚀 (우선순위: 낮음)
- [ ] **컨테이너화**
  - Docker 이미지 생성
  - 의존성 관리 최적화
  - 환경 일관성 확보

- [ ] **API 서비스화**
  - REST API 인터페이스
  - 비동기 작업 큐
  - 인증 및 권한 관리

- [ ] **클라우드 배포**
  - 서버리스 아키텍처
  - 자동 스케일링
  - 모니터링 및 알림

### 11. 연구 확장 🔬 (우선순위: 낮음)
- [ ] **새로운 아키텍처 실험**
  - Hierarchical 그래프 구조
  - Memory-augmented 추론
  - Multi-modal 입력 지원

- [ ] **벤치마크 확장**
  - 새로운 평가 지표
  - Cross-dataset 일반화
  - Human evaluation 연구

## 우선순위별 실행 계획

### Week 1: 즉시 실행 🏃‍♂️
1. 환경 설정 및 기본 테스트
2. MMQA 샘플 데이터 실행
3. 성능 벤치마킹 시작

### Week 2-3: 안정화 🔧
1. 오류 처리 강화
2. 보상 함수 개선
3. 설정 최적화

### Week 4-6: 확장 📈
1. 추가 데이터셋 지원
2. 성능 최적화
3. 고급 기능 추가

### Beyond: 발전 🚀
1. 프로덕션 배포 준비
2. 연구 확장 및 실험
3. 커뮤니티 기여

## 리소스 요구사항

### 기술 요구사항
- Python 3.8+
- OpenAI API 접근
- 16GB+ RAM (대용량 데이터 처리 시)
- GPU (선택사항, 로컬 모델 사용 시)

### 데이터 요구사항
- MMQA 데이터셋 (31KB)
- WTQ, TAT, CRT, SciTab 데이터셋 (확장 시)
- 평가 기준 데이터

### 인력 요구사항
- 개발자 1명 (주요 구현)
- 연구자 1명 (평가 및 분석)
- 테스터 1명 (QA 및 검증)

## 성공 지표

### 기술적 지표
- **정확도**: MMQA에서 70%+ EM 달성
- **성능**: 질문당 20초 이내 처리
- **안정성**: 99%+ 성공적 실행
- **확장성**: 1000+ 질문 배치 처리

### 비즈니스 지표
- **사용성**: 30분 이내 설치 및 실행
- **문서화**: 95%+ 기능 문서화
- **테스트**: 90%+ 코드 커버리지
- **유지보수성**: 24시간 이내 버그 수정

## 리스크 및 대응

### 기술적 리스크
- **API 비용**: 토큰 사용량 최적화 필요
- **성능 저하**: 프로파일링 및 최적화
- **호환성 문제**: 버전 고정 및 테스트

### 일정 리스크
- **복잡성 증가**: 단계별 검증
- **의존성 문제**: 대안 방안 준비
- **리소스 부족**: 우선순위 조정

## 연락처 및 지원
- **개발 문의**: GitHub Issues
- **기술 지원**: README 및 문서 참조
- **기여 방법**: CONTRIBUTING.md 참조

---

이 TODO 리스트는 현재 구현 상태를 기반으로 작성되었으며, 프로젝트 진행에 따라 업데이트될 예정입니다.