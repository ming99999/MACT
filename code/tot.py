""" Utility classes and functions related to MACT (NAACL 2025). 

Copyright (c) 2025 Robert Bosch GmbH 


This program is free software: you can redistribute it and/or modify 

it under the terms of the GNU Affero General Public License as published 

by the Free Software Foundation, either version 3 of the License, or 

(at your option) any later version. 

This program is distributed in the hope that it will be useful, 

but WITHOUT ANY WARRANTY; without even the implied warranty of 

MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the 

GNU Affero General Public License for more details. 

You should have received a copy of the GNU Affero General Public License 

along with this program.  If not, see <https://www.gnu.org/licenses/>. 

"""

import re
import string
import os
from typing import List, Union, Literal
from enum import Enum
from llm import UnifiedLLM
from config import llm_config


vote_prompt_as = '''Given a question, a table, past reasonings and several current intermediate reasoning paths, decide which current reasoning path is the most promising in terms of solving the question. Analyze each path in detail, then conclude in the last line "The best path is {s}", where s the integer id of the path.
'''

vote_prompt_obs = '''Given an instruction, a table and several results generated by following the instruction, decide which result is the most correct. Analyze each result in detail, then conclude in the last line "The best result is {s}", where s the integer id of the result.
'''

score_prompt = '''Analyze the following passage, then at the last line conclude "Thus the coherency score is {s}", where s is an integer from 1 to 10.
'''


# _ = load_dotenv(find_dotenv())
# token_provider = get_bearer_token_provider(
#     DefaultAzureCredential(
#         exclude_managed_identity_credential=True
#     ), "https://cognitiveservices.azure.com/.default"
# )
# client = AzureOpenAI(
#     azure_ad_token_provider=token_provider,
#     azure_endpoint=""
# )


# all_input_token, all_output_token = 0, 0

def get_completion(prompt, model="gpt-4-turbo", client=None):
    """Get completion using unified LLM interface."""
    print(f"using {model}!")
    
    # Use unified LLM approach
    llm = UnifiedLLM(model)
    response = llm(prompt, num_return_sequences=1, max_tokens=1000, temperature=0.0)
    
    # Simplified token counting
    input_token_num = len(prompt) // 4
    output_token_num = len(response[0]) // 4 if response else 0
    
    return response[0] if response else "", input_token_num, output_token_num


def llm_reward(reasoning_paths, vote_prompt, model_type="closed", tokenizer=None, model_name="", model=None):
    """Get LLM reward using unified interface."""
    prompt = vote_prompt + reasoning_paths
    
    if model_type == "closed":
        outputs, input_tokens_num, output_tokens_num = get_completion(prompt)
    elif model_type == "open":
        # Use unified LLM for open-source models
        llm = UnifiedLLM(model_name)
        outputs = llm(prompt, num_return_sequences=1, return_prob=False)
        outputs = outputs[0] if outputs else ""
        input_tokens_num = len(prompt) // 4
        output_tokens_num = len(outputs) // 4
    else:
        outputs = ""
        input_tokens_num = 0
        output_tokens_num = 0
        
    return outputs, input_tokens_num, output_tokens_num
