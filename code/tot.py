""" Utility classes and functions related to MACT (NAACL 2025). 

Copyright (c) 2025 Robert Bosch GmbH 


This program is free software: you can redistribute it and/or modify 

it under the terms of the GNU Affero General Public License as published 

by the Free Software Foundation, either version 3 of the License, or 

(at your option) any later version. 

This program is distributed in the hope that it will be useful, 

but WITHOUT ANY WARRANTY; without even the implied warranty of 

MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the 

GNU Affero General Public License for more details. 

You should have received a copy of the GNU Affero General Public License 

along with this program.  If not, see <https://www.gnu.org/licenses/>. 

"""

import re
import string
import os
from typing import List, Union, Literal
from enum import Enum
import tiktoken
import pandas as pd
from langchain import OpenAI, Wikipedia
from langchain.llms.base import BaseLLM
from langchain.chat_models import ChatOpenAI
from langchain.agents import Tool
from langchain.chat_models.base import BaseChatModel
from langchain.schema import (
    SystemMessage,
    HumanMessage,
    AIMessage,
)
from langchain.agents.react.base import DocstoreExplorer
from langchain.docstore.base import Docstore
from langchain.prompts import PromptTemplate
from llm import OpenSourceLLM

from dotenv import load_dotenv, find_dotenv
from azure.identity import get_bearer_token_provider, DefaultAzureCredential
from openai import AzureOpenAI


vote_prompt_as = '''Given a question, a table, past reasonings and several current intermediate reasoning paths, decide which current reasoning path is the most promising in terms of solving the question. Analyze each path in detail, then conclude in the last line "The best path is {s}", where s the integer id of the path.
'''

vote_prompt_obs = '''Given an instruction, a table and several results generated by following the instruction, decide which result is the most correct. Analyze each result in detail, then conclude in the last line "The best result is {s}", where s the integer id of the result.
'''

score_prompt = '''Analyze the following passage, then at the last line conclude "Thus the coherency score is {s}", where s is an integer from 1 to 10.
'''


# _ = load_dotenv(find_dotenv())
# token_provider = get_bearer_token_provider(
#     DefaultAzureCredential(
#         exclude_managed_identity_credential=True
#     ), "https://cognitiveservices.azure.com/.default"
# )
# client = AzureOpenAI(
#     azure_ad_token_provider=token_provider,
#     azure_endpoint=""
# )


# all_input_token, all_output_token = 0, 0

def get_completion(prompt, model="gpt-4-turbo", client=None):
    print(f"using {model}!")
    messages = [{"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.,
        max_tokens=1000,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
    )
    input_token_num = response.usage.prompt_tokens
    output_token_num = response.usage.completion_tokens
    return response.choices[0].message.content, input_token_num, output_token_num


def llm_reward(reasoning_paths, vote_prompt, model_type="closed", tokenizer=None, model_name="", model=None):
    prompt = vote_prompt + reasoning_paths
    if model_type == "closed":
        outputs, input_tokens_num, output_tokens_num = get_completion(prompt)
    elif model_type == "open":
        assert tokenizer, model
        llm = OpenSourceLLM(
            model_name=model_name,
            model=None,
            vllm=model.vllm,
            tokenizer=tokenizer
        )
        outputs = llm(prompt, num_return_sequences=1, return_prob=False)
    return outputs, "", ""
