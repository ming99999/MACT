# TODO: MMQA Benchmark Compliance & Subtask Generation (2025-10-11)

## âœ… Completed Phase 1: Input Compliance Fixes

### Phase 1A: Remove FK/PK from Input (COMPLETED âœ…)
**Problem**: FK/PK were incorrectly used as input instead of output targets
**Solution**: Removed FK/PK from all state definitions and context generation

**Files Modified**:
- âœ… `src/mact_langgraph/state.py` - Removed FK/PK fields
- âœ… `src/mact_langgraph/utils/mmqa_utils.py` - Context uses only table_names
- âœ… `main.py` - Removed FK/PK from initialization
- âœ… `code/tqa_mmqa.py` - Fixed context creation

**Test Results**:
- âœ… LangGraph version tested with gpt-3.5-turbo
- âœ… Achieved 28.6% EM (honest baseline)
- âœ… -14.3%p drop from non-compliant version (42.9%)

### Phase 1B: Create Evaluation Infrastructure (COMPLETED âœ…)
**Files Created**:
- âœ… `code/evaluate_mmqa.py` - Comprehensive evaluation script
- âœ… `logs_ai/MMQA_COMPLIANCE_FIX.md` - Fix documentation
- âœ… `logs_ai/MMQA_COMPLIANCE_FINAL_RESULTS.md` - Results report

---

## ðŸš§ Phase 2: Subtask Output Generation (NEW - TO BE IMPLEMENTED)

### Current Status Assessment

**What Works**:
- âœ… MACT framework generates **Answer** (SUBTASK 1) effectively
- âœ… Answer EM metric (28.6%) aligns with paper benchmarks
- âœ… Reasoning history (scratchpad/execution_log) contains rich information

**What's Missing**:
- âŒ SQL generation (SUBTASK 2) - Not implemented
- âŒ Primary Key prediction (SUBTASK 3) - Rudimentary heuristics only
- âŒ Foreign Key prediction (SUBTASK 4) - Rudimentary heuristics only

### Problem Analysis

**Root Cause**: MACT was designed for Question â†’ Answer, not for generating SQL/FK/PK
- Original MACT framework focuses on answer extraction
- Code generation produces pandas operations, not SQL
- FK/PK relationships inferred during execution but not explicitly output

**Opportunity**: Rich reasoning history can be leveraged
- `step_history`: Records all actions and observations
- `scratchpad`: Contains reasoning chain
- `execution_log`: Records tool executions
- Generated code: Contains JOIN/merge operations indicating FK relationships

---

## ðŸ“‹ Phase 2 Implementation Plan

### Option 1: Post-Processing with LLM (RECOMMENDED â­)

**Approach**: After answer generation, use LLM to extract SQL/FK/PK from history

**Advantages**:
- âœ… Minimal changes to core MACT framework
- âœ… Leverages existing reasoning history
- âœ… Can use same or smaller LLM for extraction
- âœ… Maintains MACT's strength (answer generation)

**Implementation Steps**:

#### Step 2.1: Create Subtask Extraction Utilities
**File**: `src/mact_langgraph/utils/subtask_extraction.py` (NEW)

```python
async def extract_sql_from_history(
    question: str,
    history: List[Dict],
    tables_info: List[Dict],
    llm_model: str
) -> str:
    """
    Extract SQL query from reasoning history using LLM.

    Args:
        question: Original question
        history: Step-by-step execution history
        tables_info: Table metadata
        llm_model: Model to use for extraction

    Returns:
        Generated SQL query
    """
    # Prompt:
    # "Given the question, tables, and reasoning history,
    #  generate the SQL query that answers the question."
    pass

async def extract_foreign_keys_from_history(
    history: List[Dict],
    tables_info: List[Dict],
    llm_model: str
) -> List[str]:
    """
    Extract predicted foreign keys from reasoning history.

    Looks for:
    - JOIN operations in generated code
    - Merge operations with column mappings
    - Explicit FK mentions in reasoning
    """
    pass

async def extract_primary_keys_from_tables(
    tables_info: List[Dict],
    history: List[Dict],
    llm_model: str
) -> List[str]:
    """
    Predict primary keys from table structure + usage patterns.

    Combines:
    - Column name patterns (_id, id)
    - Usage in JOIN operations
    - Uniqueness constraints inferred from data
    """
    pass
```

#### Step 2.2: Add Post-Processing Node to Graph
**File**: `src/mact_langgraph/nodes/subtask_nodes.py` (NEW)

```python
async def subtask_generation_node(state: MACTState) -> MACTState:
    """
    Generate SQL, FK, PK predictions after answer is determined.

    This node runs AFTER the answer is finalized.
    """
    from ..utils.subtask_extraction import (
        extract_sql_from_history,
        extract_foreign_keys_from_history,
        extract_primary_keys_from_tables
    )

    # Extract subtasks using LLM
    predicted_sql = await extract_sql_from_history(
        question=state["question"],
        history=state["step_history"],
        tables_info=state["tables"],
        llm_model=state["code_model"]  # Reuse code model
    )

    predicted_fks = await extract_foreign_keys_from_history(
        history=state["step_history"],
        tables_info=state["tables"],
        llm_model=state["code_model"]
    )

    predicted_pks = await extract_primary_keys_from_tables(
        tables_info=state["tables"],
        history=state["step_history"],
        llm_model=state["code_model"]
    )

    return {
        **state,
        "predicted_sql": predicted_sql,
        "predicted_foreign_keys": predicted_fks,
        "predicted_primary_keys": predicted_pks
    }
```

#### Step 2.3: Update State Schema
**File**: `src/mact_langgraph/state.py`

```python
class MACTState(TypedDict):
    # ... existing fields ...

    # Subtask predictions (MMQA compliance - outputs only)
    predicted_sql: Optional[str]
    predicted_primary_keys: List[str]
    predicted_foreign_keys: List[str]
```

#### Step 2.4: Integrate into Graph
**File**: `src/mact_langgraph/graph.py`

```python
# Add subtask generation node AFTER answer is finalized
graph.add_node("generate_subtasks", subtask_generation_node)

# Connect: finish_node â†’ generate_subtasks â†’ END
graph.add_edge("finish", "generate_subtasks")
graph.add_edge("generate_subtasks", END)
```

#### Step 2.5: Update Output Format
**File**: `main.py`

```python
result = {
    # ... existing fields ...

    # SUBTASK 1: Answer
    'predicted': final_state.get('final_answer', ''),

    # SUBTASK 2: SQL
    'pred_sql': final_state.get('predicted_sql', ''),

    # SUBTASK 3: Primary Keys
    'pred_primary_keys': final_state.get('predicted_primary_keys', []),

    # SUBTASK 4: Foreign Keys
    'pred_foreign_keys': final_state.get('predicted_foreign_keys', [])
}
```

---

### Option 2: Hybrid Approach (ALTERNATIVE)

**Approach**: Mix rule-based extraction + LLM refinement

**Step 2A**: Rule-based initial extraction
- Parse pandas code for JOIN/merge operations â†’ FK candidates
- Identify _id columns â†’ PK candidates
- Convert pandas operations to SQL templates

**Step 2B**: LLM refinement
- Use LLM to validate and refine extracted SQL
- Use LLM to confirm FK/PK predictions
- Generate missing parts

**Advantages**:
- Faster than pure LLM approach
- More accurate than pure rules
- Good balance of speed/accuracy

---

### Option 3: Framework Redesign (NOT RECOMMENDED)

**Approach**: Modify MACT to explicitly generate SQL/FK/PK during reasoning

**Why Not Recommended**:
- âŒ Major changes to core framework
- âŒ May degrade answer generation performance
- âŒ MACT's strength is answer extraction, not SQL generation
- âŒ Would need to retrain/re-prompt all components

---

## ðŸ“ Detailed Implementation Tasks (Option 1 - Recommended)

### Task 2.1: Create Subtask Extraction Utilities
**File**: `src/mact_langgraph/utils/subtask_extraction.py`

- [ ] Implement `extract_sql_from_history()`
  - [ ] Design prompt template for SQL extraction
  - [ ] Parse history to extract relevant operations
  - [ ] Convert pandas code patterns to SQL
  - [ ] Use LLM to generate complete SQL query

- [ ] Implement `extract_foreign_keys_from_history()`
  - [ ] Design prompt template for FK prediction
  - [ ] Parse JOIN/merge operations
  - [ ] Extract column mappings (left_on, right_on, on)
  - [ ] Use LLM to validate FK relationships

- [ ] Implement `extract_primary_keys_from_tables()`
  - [ ] Design prompt template for PK prediction
  - [ ] Analyze column names for _id patterns
  - [ ] Check which columns used in JOIN operations
  - [ ] Use LLM to confirm PK selections

### Task 2.2: Create Subtask Generation Node
**File**: `src/mact_langgraph/nodes/subtask_nodes.py`

- [ ] Implement `subtask_generation_node()`
  - [ ] Call all 3 extraction functions
  - [ ] Handle errors gracefully
  - [ ] Log subtask generation process
  - [ ] Update state with predictions

### Task 2.3: Update State Schema
**File**: `src/mact_langgraph/state.py`

- [ ] Add `predicted_sql: Optional[str]` field
- [ ] Add `predicted_primary_keys: List[str]` field
- [ ] Add `predicted_foreign_keys: List[str]` field
- [ ] Update `create_initial_state()` to initialize new fields

### Task 2.4: Integrate into Graph
**File**: `src/mact_langgraph/graph.py`

- [ ] Import `subtask_generation_node`
- [ ] Add node to graph after finish_node
- [ ] Update edge connections
- [ ] Test graph execution flow

### Task 2.5: Update Main Entry Point
**File**: `main.py`

- [ ] Update result dictionary to include subtask predictions
- [ ] Update `save_prediction_item()` to save subtask outputs
- [ ] Ensure backward compatibility with existing code

### Task 2.6: Enhance Evaluation Script
**File**: `code/evaluate_mmqa.py`

- [ ] Add SQL evaluation metrics (Rouge-1, Rouge-L, BLEU)
  - [ ] Install/import rouge_score library
  - [ ] Implement BLEU calculation
  - [ ] Add SQL preprocessing (normalization)

- [ ] Improve PK/FK evaluation
  - [ ] Handle partial matches
  - [ ] Calculate precision/recall/F1
  - [ ] Add detailed error analysis

---

## ðŸ§ª Testing Plan

### Test 2.1: Unit Tests for Extraction Functions
**File**: `tests/test_subtask_extraction.py` (NEW)

```python
async def test_extract_sql_simple():
    """Test SQL extraction from simple JOIN operation."""
    pass

async def test_extract_foreign_keys():
    """Test FK extraction from merge operations."""
    pass

async def test_extract_primary_keys():
    """Test PK prediction from table structure."""
    pass
```

### Test 2.2: Integration Test
**Command**:
```bash
python main.py \
  --dataset_path ../datasets_examples/mmqa_samples.json \
  --plan_model "gpt-3.5-turbo" \
  --code_model "gpt-3.5-turbo" \
  --plan_sample 3 \
  --code_sample 3 \
  --debug \
  --debug_limit 3 \
  --output_dir test_subtask_generation
```

**Expected Output**:
```json
{
  "id": 1,
  "question": "...",
  "predicted": "Treasury, 115897",
  "pred_sql": "SELECT d.name, d.num_employees FROM department d JOIN management m ON d.department_id = m.department_id WHERE m.temporary_acting = 'Yes' ORDER BY d.num_employees DESC LIMIT 1",
  "pred_primary_keys": ["department_id", "head_id"],
  "pred_foreign_keys": ["department_id"]
}
```

### Test 2.3: Full Evaluation
```bash
python code/evaluate_mmqa.py \
  --predictions test_subtask_generation/predictions_*.jsonl \
  --output test_subtask_generation/evaluation_results.json \
  --verbose
```

**Expected Metrics**:
- Answer EM: ~28.6% (should remain similar)
- SQL Rouge-1: TBD (depends on extraction quality)
- PKS Accuracy: TBD (likely 40-60% initially)
- FKS Accuracy: TBD (likely 30-50% initially)

---

## ðŸ“Š Success Criteria

### Minimum Viable Product (MVP)
- [ ] SQL generation implemented (even if imperfect)
- [ ] FK/PK prediction implemented (even if basic)
- [ ] All 4 subtasks included in output format
- [ ] Evaluation metrics for all 4 subtasks working
- [ ] Answer EM remains ~28.6% (no degradation)

### Target Performance
- [ ] SQL Rouge-1 > 0.3
- [ ] PKS Accuracy > 50%
- [ ] FKS Accuracy > 40%
- [ ] Total runtime increase < 20%

### Documentation
- [ ] Update `MMQA_COMPLIANCE_FIX.md` with subtask implementation
- [ ] Update `MMQA_COMPLIANCE_FINAL_RESULTS.md` with new metrics
- [ ] Add examples of generated SQL/FK/PK in documentation

---

## â±ï¸ Estimated Timeline

**Phase 2A: Extraction Utilities** (2-3 hours)
- Task 2.1: Create subtask_extraction.py
- Design prompts for SQL/FK/PK extraction

**Phase 2B: Graph Integration** (1-2 hours)
- Task 2.2-2.4: Add node, update state, integrate into graph

**Phase 2C: Output & Evaluation** (1 hour)
- Task 2.5-2.6: Update main.py and evaluation script

**Phase 2D: Testing** (2-3 hours)
- Unit tests, integration test, full evaluation
- Debug and refine extraction prompts

**Total Estimated Time**: 6-9 hours

---

## ðŸŽ¯ Key Decisions

### Decision 1: Post-Processing vs Integrated Generation
**Choice**: Post-processing (Option 1)
**Rationale**:
- Preserves MACT's core strength (answer generation)
- Minimal risk to existing performance
- Easier to implement and debug
- Can iterate on subtask extraction independently

### Decision 2: LLM-based vs Rule-based Extraction
**Choice**: LLM-based with rule-based hints (Hybrid)
**Rationale**:
- Pure rules too brittle for varied code patterns
- Pure LLM may be slow/expensive
- Hybrid: rules extract candidates, LLM refines

### Decision 3: Same Model vs Separate Model for Subtasks
**Choice**: Reuse code_model for subtask extraction
**Rationale**:
- Simplifies configuration
- code_model already proven capable
- Can optimize later if needed

---

## ðŸ“ Files to Create/Modify

### New Files
- [ ] `src/mact_langgraph/utils/subtask_extraction.py`
- [ ] `src/mact_langgraph/nodes/subtask_nodes.py`
- [ ] `tests/test_subtask_extraction.py`

### Modified Files
- [ ] `src/mact_langgraph/state.py` (add subtask fields)
- [ ] `src/mact_langgraph/graph.py` (add subtask node)
- [ ] `main.py` (update output format)
- [ ] `code/evaluate_mmqa.py` (add SQL metrics)
- [ ] `logs_ai/MMQA_COMPLIANCE_FIX.md` (document implementation)
- [ ] `logs_ai/MMQA_COMPLIANCE_FINAL_RESULTS.md` (update results)

---

## ðŸ”„ Progress Tracking

**Phase 1: Input Compliance** âœ… COMPLETED
- [x] Remove FK/PK from input
- [x] Test with honest baseline
- [x] Create evaluation infrastructure

**Phase 2: Subtask Generation** ðŸš§ IN PROGRESS
- [ ] Task 2.1: Create extraction utilities
- [ ] Task 2.2: Create subtask node
- [ ] Task 2.3: Update state schema
- [ ] Task 2.4: Integrate into graph
- [ ] Task 2.5: Update main entry point
- [ ] Task 2.6: Enhance evaluation script

**Phase 3: Testing & Validation** â³ PENDING
- [ ] Unit tests
- [ ] Integration test
- [ ] Full evaluation
- [ ] Performance analysis

**Phase 4: Documentation** â³ PENDING
- [ ] Update MMQA_COMPLIANCE_FIX.md
- [ ] Update MMQA_COMPLIANCE_FINAL_RESULTS.md
- [ ] Add usage examples

---

## ðŸ’¡ Implementation Notes

### SQL Generation Strategy
```
Input: Question + Tables + Reasoning History
Process:
1. Extract all pandas operations from history
2. Identify JOIN/merge patterns â†’ SQL JOIN
3. Identify filter conditions â†’ SQL WHERE
4. Identify groupby/agg â†’ SQL GROUP BY
5. Use LLM to combine into coherent SQL query

Prompt Template:
"Given this question about tables {tables}, and the reasoning history
showing pandas operations {operations}, generate the equivalent SQL query."
```

### FK Extraction Strategy
```
Input: Reasoning History + Tables
Process:
1. Find all merge() operations in generated code
2. Extract: merge(left_df, right_df, on='X') â†’ FK candidate: X
3. Extract: merge(left_on='A', right_on='B') â†’ FK: Aâ†’B relationship
4. Use LLM to validate: "Are these valid foreign keys?"

Heuristics:
- Column ending in _id used in JOIN â†’ likely FK
- Same column name across tables â†’ likely FK
```

### PK Extraction Strategy
```
Input: Tables + Reasoning History
Process:
1. Find columns ending in _id, named 'id', or 'primary_key'
2. Find columns used as left_on/right_on in JOINs
3. Check if column appears unique in data
4. Use LLM to confirm: "Which columns are primary keys?"

Heuristics:
- Column named exactly "id" â†’ likely PK
- Column named "{table}_id" â†’ likely PK of that table
- Column used as join key consistently â†’ likely PK
```

---

## ðŸš€ Next Steps

1. **Immediate**: Create `subtask_extraction.py` with extraction functions
2. **Short-term**: Integrate subtask generation into graph
3. **Medium-term**: Test and refine extraction prompts
4. **Long-term**: Optimize performance and accuracy

**Ready to begin Phase 2 implementation!** ðŸŽ¯
