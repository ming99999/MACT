# Batch API Debug Success Log

**ë‚ ì§œ**: 2025-10-02 (2025-10-03 00:09 KST)
**ë¬¸ì œ í•´ê²°**: RunPod vLLM Batch API ì‘ë™ í™•ì¸

---

## ğŸ” ë¬¸ì œ ë°œê²¬ ê³¼ì •

### Issue #1: Batch API íƒ€ì„ì•„ì›ƒ
**ì¦ìƒ**:
```
Command timed out after 3m 0s
```

**ì›ì¸**: Cold start - RunPod vLLM endpointê°€ ì˜¤ë«ë™ì•ˆ ì‚¬ìš©í•˜ì§€ ì•Šì•„ ì´ˆê¸°í™” ì‹œê°„ í•„ìš”

**í•´ê²°**: Timeout 300ì´ˆ â†’ 600ì´ˆ(10ë¶„)ë¡œ ì¦ê°€

---

### Issue #2: Response Choices = None
**ì¦ìƒ**:
```
ğŸ” DEBUG Planning: Choices=None
ğŸ” DEBUG Planning: Choices length=None
âš ï¸ Batch API returned no valid responses
```

**ì›ì¸**: ëª¨ë¸ëª… ë¶ˆì¼ì¹˜
```json
{
  "object": "error",
  "code": 404,
  "message": "The model `gpt-3.5-turbo` does not exist."
}
```

**ë¶„ì„**:
- RunPod vLLM endpointëŠ” `gpt-3.5-turbo` ëª¨ë¸ëª…ì„ ì¸ì‹í•˜ì§€ ëª»í•¨
- vLLMì€ ë¡œë“œëœ ì‹¤ì œ ëª¨ë¸ ì´ë¦„ì„ ì‚¬ìš©í•´ì•¼ í•¨
- ì´ì „ í…ŒìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©í•œ `Qwen/Qwen3-8B`ê°€ ì‹¤ì œ ëª¨ë¸ëª…

---

## âœ… í•´ê²° ë°©ë²•

### 1. Timeout ì¦ê°€
```python
# Before
client = AsyncOpenAI(
    api_key=api_key,
    base_url=base_url,
    timeout=300.0  # 5 minutes
)

# After
client = AsyncOpenAI(
    api_key=api_key,
    base_url=base_url,
    timeout=600.0  # 10 minutes for cold start
)
```

### 2. ì˜¬ë°”ë¥¸ ëª¨ë¸ëª… ì‚¬ìš©
```bash
# Before (ì‹¤íŒ¨)
--plan_model "gpt-3.5-turbo" --code_model "gpt-3.5-turbo"

# After (ì„±ê³µ)
--plan_model "Qwen/Qwen3-8B" --code_model "Qwen/Qwen3-8B"
```

---

## ğŸ¯ ì„±ê³µ í™•ì¸

### Batch API ì •ìƒ ì‘ë™ ë¡œê·¸
```
ğŸ” DEBUG Planning: Attempting batch API with n=3
ğŸ” DEBUG Planning: API base_url=https://api.runpod.ai/v2/ktvaoabldmsjfk/openai/v1
ğŸ” DEBUG Planning: Calling batch API with model=Qwen/Qwen3-8B
ğŸ” DEBUG Planning: Response received, type=<class 'openai.types.chat.chat_completion.ChatCompletion'>
```

### Response êµ¬ì¡°
```python
ChatCompletion(
    id='chatcmpl-5bdc382de1ff459b9fa6b9cf56b03d68',
    choices=[
        Choice(index=0, finish_reason='stop', message=ChatCompletionMessage(content='...')),
        Choice(index=1, finish_reason='length', message=ChatCompletionMessage(content='...')),
        Choice(index=2, finish_reason='stop', message=ChatCompletionMessage(content='...'))
    ],
    created=1759417801,
    model='Qwen/Qwen3-8B',
    usage=CompletionUsage(
        completion_tokens=3007,
        prompt_tokens=846,
        total_tokens=3853
    )
)
```

**í•µì‹¬ í™•ì¸ ì‚¬í•­**:
- âœ… `choices` í•„ë“œ: 3ê°œì˜ Choice ê°ì²´ í¬í•¨
- âœ… ê° Choiceë§ˆë‹¤ `message.content` ì¡´ì¬
- âœ… `n=3` íŒŒë¼ë¯¸í„°ê°€ ì •ìƒ ì‘ë™
- âœ… 1ë²ˆì˜ API í˜¸ì¶œë¡œ 3ê°œ ìƒ˜í”Œ ìƒì„±

---

## ğŸ“Š íš¨ê³¼

### Code Generation (Retriever Tool)
```
ğŸ” DEBUG: Attempting batch API with n=3
ğŸ¯ Batch API: Generated 3 samples in 1 call (Original MACT style)
```

### Action Planning
```
ğŸ” DEBUG Planning: Attempting batch API with n=3
ğŸ¯ Planning Batch API: Generated 3 plans in 1 call
```

### ì„±ëŠ¥ ê°œì„ 
- **API í˜¸ì¶œ íšŸìˆ˜**: 3ë²ˆ â†’ 1ë²ˆ (1/3)
- **ë¹„ìš©**: 66% ì ˆê°
- **ì†ë„**: ì˜ˆìƒ 3ë°° ë¹ ë¦„ (ë„¤íŠ¸ì›Œí¬ ì™•ë³µ ì‹œê°„ ì ˆì•½)
- **í’ˆì§ˆ**: Correlated samplesë¡œ consistency reward í–¥ìƒ

---

## ğŸ”„ ì ìš©ëœ ìˆ˜ì •

### íŒŒì¼: `src/mact_langgraph/nodes/tool_nodes.py`
```python
async def generate_code_batch(llm, prompt: str, n: int, model_name: str = None) -> List[str]:
    try:
        if hasattr(llm, 'client'):
            from openai import AsyncOpenAI

            api_key = llm.openai_api_key.get_secret_value()  # SecretStr ì²˜ë¦¬
            base_url = llm.openai_api_base

            client = AsyncOpenAI(
                api_key=api_key,
                base_url=base_url,
                timeout=600.0  # Cold start ëŒ€ì‘
            )

            response = await client.chat.completions.create(
                model=model_name or llm.model_name,
                messages=[{"role": "user", "content": prompt}],
                n=n,  # â­ Batch generation
                temperature=0.6,
                max_tokens=2000
            )

            if response and response.choices:
                codes = [choice.message.content for choice in response.choices if choice.message.content]
                if codes:
                    print(f"ğŸ¯ Batch API: Generated {len(codes)} samples in 1 call")
                    return codes
```

### íŒŒì¼: `src/mact_langgraph/nodes/core_nodes.py`
```python
async def generate_plan_batch(llm, prompt: str, n: int, model_name: str = None) -> List[str]:
    # ë™ì¼í•œ íŒ¨í„´
```

---

## ğŸ“ êµí›ˆ

### 1. vLLM Endpoint íŠ¹ì„±
- vLLMì€ OpenAI APIë¥¼ emulateí•˜ì§€ë§Œ ì™„ì „íˆ ë™ì¼í•˜ì§€ ì•ŠìŒ
- ëª¨ë¸ëª…ì€ vLLMì— ë¡œë“œëœ ì‹¤ì œ ëª¨ë¸ ì´ë¦„ì„ ì‚¬ìš©í•´ì•¼ í•¨
- `gpt-3.5-turbo` ê°™ì€ OpenAI ëª¨ë¸ëª…ì€ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ

### 2. Cold Start ê³ ë ¤
- ì˜¤ëœë§Œì— ì‚¬ìš©í•˜ëŠ” endpointëŠ” ì´ˆê¸°í™” ì‹œê°„ í•„ìš”
- Timeoutì„ ë„‰ë„‰í•˜ê²Œ ì„¤ì • (ìµœì†Œ 10ë¶„)
- ì²« í˜¸ì¶œì€ ëŠë¦¬ì§€ë§Œ ì´í›„ í˜¸ì¶œì€ ë¹ ë¦„

### 3. Batch API ì§€ì› í™•ì¸
- RunPod vLLMì€ `n` íŒŒë¼ë¯¸í„° ì§€ì›
- Response êµ¬ì¡°ëŠ” OpenAIì™€ ë™ì¼
- Choices ë°°ì—´ì— nê°œì˜ ìƒ˜í”Œ ë°˜í™˜

### 4. ë””ë²„ê¹… ë¡œê·¸ì˜ ì¤‘ìš”ì„±
- Response ì „ì²´ êµ¬ì¡° ì¶œë ¥ìœ¼ë¡œ ë¬¸ì œ ì •í™•íˆ íŒŒì•…
- Error message í™•ì¸ìœ¼ë¡œ ëª¨ë¸ëª… ë¬¸ì œ ë°œê²¬
- ë‹¨ê³„ë³„ ë””ë²„ê·¸ ë¡œê·¸ë¡œ ì–´ëŠ ë¶€ë¶„ì—ì„œ ì‹¤íŒ¨í•˜ëŠ”ì§€ í™•ì¸

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **Full Dataset Test** - í˜„ì¬ ì§„í–‰ ì¤‘
   - 21 questions with Qwen/Qwen3-8B
   - Batch API + Hybrid Voting íš¨ê³¼ ì¸¡ì •
   - ì˜ˆìƒ ëª©í‘œ: 42.9% â†’ 46-54%

2. **Debug Logs ì œê±°**
   - ì„±ê³µ í™•ì¸ í›„ ë””ë²„ê·¸ print ë¬¸ ì œê±°
   - ë˜ëŠ” logger ë¡œ ëŒ€ì²´

3. **ì„±ëŠ¥ ë¶„ì„**
   - Batch API vs abatch ì†ë„ ë¹„êµ
   - Hybrid voting íš¨ê³¼ ì •ëŸ‰í™”
   - Original MACT 58.8%ì™€ gap ë¶„ì„

---

**ì‘ì„± ì™„ë£Œ**: 2025-10-03 00:15 KST
**ìƒíƒœ**: Batch API ì •ìƒ ì‘ë™ í™•ì¸, Full test ì§„í–‰ ì¤‘
