# Phase 1 ì„±ëŠ¥ ì €í•˜ ê·¼ë³¸ ì›ì¸ ë¶„ì„

**ë‚ ì§œ**: 2025-10-02 06:10
**ë¬¸ì œ**: Phase 1 êµ¬í˜„ í›„ 42.9% â†’ 33.3% ì„±ëŠ¥ ì €í•˜ (-9.5%p)

---

## ğŸ” ë°œê²¬ëœ í•µì‹¬ ì°¨ì´ì 

### Issue #1: LLM í˜¸ì¶œ ë°©ì‹ì˜ ê·¼ë³¸ì  ì°¨ì´

#### Original MACT ë°©ì‹
```python
# agents.py:283 - í•œ ë²ˆì˜ LLM í˜¸ì¶œë¡œ ì—¬ëŸ¬ ìƒ˜í”Œ ìƒì„±
codes = self.llm(prompt, num_return_sequences=max_attempt, return_prob=False)
# â†’ OpenAI APIì˜ n íŒŒë¼ë¯¸í„° ì‚¬ìš©
# â†’ 1ë²ˆì˜ API í˜¸ì¶œë¡œ 5ê°œ ìƒ˜í”Œ ìƒì„± (ìƒê´€ê´€ê³„ ìˆìŒ)
```

#### LangGraph ë°©ì‹ (ì˜ëª»ë¨)
```python
# tool_nodes.py:125 - ì—¬ëŸ¬ ë²ˆ ë…ë¦½ì ìœ¼ë¡œ í˜¸ì¶œ
responses = await llm.abatch([prompt] * code_sample)
# â†’ 3ë²ˆì˜ ë…ë¦½ì ì¸ API í˜¸ì¶œ
# â†’ ê° ìƒ˜í”Œì´ uncorrelated (ë…ë¦½ì )
```

**ì˜í–¥**:
- âŒ **ë¹„íš¨ìœ¨**: 3ë²ˆ vs 1ë²ˆ API í˜¸ì¶œ (3ë°° ëŠë¦¼, 3ë°° ë¹„ìš©)
- âŒ **í’ˆì§ˆ ì €í•˜**: Consistency rewardê°€ ì•½í™”ë¨
  - Original: ìƒê´€ê´€ê³„ ìˆëŠ” ìƒ˜í”Œë“¤ì˜ majority voting
  - LangGraph: ë…ë¦½ ìƒ˜í”Œë“¤ì˜ voting (ì‹ ë¢°ë„ ë‚®ìŒ)

---

### Issue #2: all_observationsì˜ ì •ì²´

#### Original MACTì˜ hybrid voting ë©”ì»¤ë‹ˆì¦˜
```python
# agents.py:782-788 - Retrieve tool
new_ob = self.retriever_tool(instruction=argument)  # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
if new_ob != []:
    new_ob = [f'Observation {self.step_n}: {item}' for item in new_ob]

    # â­ í•µì‹¬: all_observationsëŠ” as_reward_fnì—ì„œ ì˜¨ë‹¤!
    if not self.long_table and not self.code_as_observation:
        new_ob += all_observations  # LLMì´ ìƒì„±í•œ observations ì¶”ê°€

    observation = Counter(new_ob).most_common(1)[0][0]  # Hybrid voting
```

#### as_reward_fnì—ì„œ all_observations ìƒì„±
```python
# agents.py:578-590 - as_consistency í•¨ìˆ˜
def as_consistency(action_thought, observations):
    # observationsëŠ” LLM ìƒ˜í”Œì—ì„œ ì¶”ì¶œí•œ ê´€ì°°ë“¤
    target_observation = Counter(observations).most_common(1)[0][0]
    # ...
    return target_thought, target_action, target_observation, observations
    # â­ observationsë¥¼ all_observationsë¡œ ë°˜í™˜!

# agents.py:754 - step í•¨ìˆ˜ì—ì„œ ì‚¬ìš©
thought, action, observation, all_observations = self.as_reward_fn(sampled)
```

**í•µì‹¬ ë°œê²¬**:
- `all_observations`ëŠ” **LLM ìƒ˜í”Œì—ì„œ íŒŒì‹±í•œ Observationë“¤**
- Action planning ë‹¨ê³„ì—ì„œ LLMì´ **ì˜ˆì¸¡í•œ** observations
- ì‹¤ì œ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ì™€ **ì˜ˆì¸¡ëœ ê´€ì°°ì„ ê²°í•©**í•˜ì—¬ voting

---

### Issue #3: LangGraph Phase 1 êµ¬í˜„ì˜ ë¬¸ì œ

#### ì˜ëª»ëœ êµ¬í˜„
```python
# tool_nodes.py:171-175 - Phase 1 ì‹œë„
if not long_table and not code_as_observation and successful_results:
    # Generate LLM observations from successful results
    for i, result in enumerate(successful_results):
        llm_obs = f"Observation {state['current_step']}: {result[:100]}"
        all_observations.append(llm_obs)
```

**ë¬¸ì œì **:
1. âŒ LLM observationsë¥¼ **ë„êµ¬ ê²°ê³¼ì—ì„œ ë³µì‚¬**
   - Original: LLMì´ **ì˜ˆì¸¡í•œ** observations ì‚¬ìš©
   - Phase 1: ë„êµ¬ ê²°ê³¼ë¥¼ ë‹¨ìˆœ ë³µì‚¬ â†’ ì¤‘ë³µ ì¹´ìš´íŒ…

2. âŒ `all_observations`ì˜ ì¶œì²˜ê°€ ì˜ëª»ë¨
   - Original: `as_reward_fn`ì˜ action planningì—ì„œ ìƒì„±
   - Phase 1: Tool executionì—ì„œ ìƒì„± (í‹€ë¦¼!)

3. âŒ Observation ì¤‘ë³µìœ¼ë¡œ voting ì™œê³¡
   ```
   Original voting:
   - Tool result: "Treasury, 115897" (3/5 ì„±ê³µ)
   - LLM prediction: "Treasury department" (2/5 ìƒ˜í”Œ)
   - Hybrid voting: 5ê°œ ê´€ì°° ì¤‘ majority

   Phase 1 voting (ì˜ëª»ë¨):
   - Tool result: "Treasury, 115897" (3/5)
   - Duplicated obs: "Observation: Treasury, 115897" (3/5)
   - Total: 6ê°œ ì¤‘ 3ê°œ ì¤‘ë³µ â†’ ì™œê³¡!
   ```

---

## ğŸ’¡ ì •í™•í•œ ìˆ˜ì • ë°©ì•ˆ

### Fix #1: Batch API ì‚¬ìš© (num_return_sequences)

**Before (LangGraph - ì˜ëª»ë¨)**:
```python
responses = await llm.abatch([prompt] * code_sample)
```

**After (Original MACT ë°©ì‹)**:
```python
# OpenAI APIì˜ n íŒŒë¼ë¯¸í„° ì‚¬ìš©
response = await llm.ainvoke(
    prompt,
    config={"configurable": {"n": code_sample}}  # í•œ ë²ˆì— ì—¬ëŸ¬ ìƒ˜í”Œ
)
# ë˜ëŠ” ì§ì ‘ OpenAI client ì‚¬ìš©
from openai import AsyncOpenAI
client = AsyncOpenAI()
response = await client.chat.completions.create(
    model=model_name,
    messages=[{"role": "user", "content": prompt}],
    n=code_sample,  # â­ í•µì‹¬: í•œ ë²ˆì— ì—¬ëŸ¬ ìƒ˜í”Œ
    temperature=0.6
)
codes = [choice.message.content for choice in response.choices]
```

---

### Fix #2: all_observationsë¥¼ action planningì—ì„œ ìƒì„±

**Step 1: Action plannerì—ì„œ observations ì¶”ì¶œ**
```python
# core_nodes.py - action_planner_node
async def action_planner_node(state: MACTState) -> MACTState:
    # ... action planning ...

    # â­ LLM ìƒ˜í”Œì—ì„œ observations íŒŒì‹±
    llm_observations = []
    for sample in raw_llm_responses:
        # "Observation N: ..." íŒ¨í„´ ì¶”ì¶œ
        obs_pattern = rf"Observation {state['current_step']}:(.*?)(?:Thought {state['current_step']+1}:|$)"
        match = re.search(obs_pattern, sample, re.DOTALL)
        if match:
            obs = f"Observation {state['current_step']}: {match.group(1).strip()}"
            llm_observations.append(obs)

    # Stateì— ì €ì¥
    return {
        **state,
        "llm_observations": llm_observations,  # ì˜ˆì¸¡ëœ observations
        # ...
    }
```

**Step 2: Tool executionì—ì„œ hybrid voting**
```python
# tool_nodes.py - retriever_tool_node
async def retriever_tool_node(state: MACTState) -> MACTState:
    # ... tool execution ...

    # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼
    tool_results = [...]  # successful_results

    # â­ Original MACT ë°©ì‹ hybrid voting
    new_ob = [f"Observation {state['current_step']}: {res}" for res in tool_results]

    long_table = state.get("long_table_op") not in [None, "ignore"]
    code_as_observation = state.get("code_as_observation", False)

    if not long_table and not code_as_observation:
        # LLMì´ ì˜ˆì¸¡í•œ observations ì¶”ê°€
        new_ob += state.get("llm_observations", [])

    # Majority voting
    observation = Counter(new_ob).most_common(1)[0][0]

    return {...}
```

---

### Fix #3: Action planningì—ì„œ batch sampling

**Before**:
```python
# core_nodes.py - ìˆœì°¨ í˜¸ì¶œ
for _ in range(plan_sample):
    response = await llm.ainvoke(prompt)
```

**After**:
```python
# Batch API ì‚¬ìš©
response = await llm.ainvoke(
    prompt,
    config={"configurable": {"n": plan_sample}}
)
```

---

## ğŸ“Š ì˜ˆìƒ íš¨ê³¼

### Fix #1 (Batch API)
- **ì†ë„**: 3ë°° ë¹ ë¦„ (1ë²ˆ vs 3ë²ˆ í˜¸ì¶œ)
- **ë¹„ìš©**: 1/3 ì ˆê°
- **í’ˆì§ˆ**: Consistency reward í–¥ìƒ
- **ì˜ˆìƒ ê°œì„ **: +3-5%p

### Fix #2 (Correct hybrid voting)
- **Voting ì •í™•ë„**: ì™œê³¡ ì œê±°
- **Observation í’ˆì§ˆ**: ì˜ˆì¸¡ + ì‹¤ì œ ê²°í•©
- **ì˜ˆìƒ ê°œì„ **: +8-12%p

### Fix #3 (Action planning batch)
- **Consistency**: ìƒê´€ê´€ê³„ ìˆëŠ” ìƒ˜í”Œ
- **Action selection**: ë” robustí•œ ì„ íƒ
- **ì˜ˆìƒ ê°œì„ **: +2-4%p

**ì´ ì˜ˆìƒ ê°œì„ **: +13-21%p
**ëª©í‘œ ì •í™•ë„**: 33.3% + 13-21% = **46-54%**

---

## ğŸ¯ ìˆ˜ì • ìš°ì„ ìˆœìœ„

### Phase 1 Revised Plan

1. **Fix #1: Batch API for code generation** (HIGH)
   - íŒŒì¼: `tool_nodes.py`
   - ì˜í–¥: ëª¨ë“  tool nodes (retrieve, operate, calculate)
   - ì˜ˆìƒ ì‹œê°„: 2-3ì‹œê°„

2. **Fix #2: LLM observations from action planning** (CRITICAL)
   - íŒŒì¼: `core_nodes.py`, `tool_nodes.py`
   - ì˜í–¥: Action planner â†’ tool execution íë¦„
   - ì˜ˆìƒ ì‹œê°„: 3-4ì‹œê°„

3. **Fix #3: Batch API for action planning** (MEDIUM)
   - íŒŒì¼: `core_nodes.py`
   - ì˜í–¥: Action generation
   - ì˜ˆìƒ ì‹œê°„: 1-2ì‹œê°„

**ì´ ì˜ˆìƒ ì‹œê°„**: 6-9ì‹œê°„ (1ì¼ ì‘ì—…)

---

## âœ… ê²€ì¦ ë°©ë²•

### Test 1: Batch API ë™ì‘ í™•ì¸
```python
# 1ë²ˆ í˜¸ì¶œë¡œ 3ê°œ ìƒ˜í”Œ ìƒì„± í™•ì¸
# ë¡œê·¸ì—ì„œ API call count ì²´í¬
```

### Test 2: LLM observations í™•ì¸
```python
# action_planner_nodeì—ì„œ observations ì¶”ì¶œ í™•ì¸
# tool_nodesì—ì„œ hybrid voting í™•ì¸
# ë¡œê·¸ì—ì„œ "Hybrid voting: tool + LLM obs" ë©”ì‹œì§€ í™•ì¸
```

### Test 3: ì„±ëŠ¥ ì¸¡ì •
```bash
# Quick test (5 questions)
python main.py --debug --debug_limit 5 ...
# Expected: 40-50% accuracy

# Full test (21 questions)
python main.py --dataset_path ...
# Expected: 46-54% accuracy
```

---

**ì‘ì„± ì™„ë£Œ**: 2025-10-02 06:10
**ë‹¤ìŒ ì‘ì—…**: Fix #1, #2, #3 ìˆœì°¨ êµ¬í˜„
**ëª©í‘œ**: 46-54% accuracy (Original MACT 58.8%ì— ê·¼ì ‘)
