# Configuration Template for MACT Unified LLM Integration
# Copy this file to .env and update with your actual API keys and endpoints

# ========================================
# OpenAI Configuration (for GPT models)
# ========================================
# Required: Your OpenAI API key
OPENAI_API_KEY=your_openai_api_key_here

# Optional: OpenAI API base URL (leave default unless using proxy)
OPENAI_API_BASE=https://api.openai.com/v1

# Optional: OpenAI Organization ID (if applicable)
# OPENAI_ORG_ID=your-organization-id

# ========================================
# RunPod vLLM Configuration (for open-source models)
# ========================================
# Required: RunPod API key for accessing your vLLM endpoint
RUNPOD_API_KEY=your_runpod_api_key_here

# Required: RunPod vLLM endpoint URL (in OpenAI-compatible format)
# Example: https://api.runpod.ai/v2/YOUR_ENDPOINT_ID/openai/v1
RUNPOD_BASE_URL=your_runpod_vllm_endpoint_url_here

# ========================================
# Usage Examples
# ========================================
# With GPT models (routes to OpenAI):
# python code/tqa.py --plan_model_name gpt-35-turbo --code_model_name gpt-35-turbo --dataset_path datasets_examples/tat.jsonl --task tat

# With open-source models (routes to RunPod vLLM):
# python code/tqa.py --plan_model_name Qwen/Qwen3-8B --code_model_name Qwen/Qwen3-8B --dataset_path datasets_examples/tat.jsonl --task tat

# Mixed models (GPT for planning, open-source for coding):
# python code/tqa.py --plan_model_name gpt-35-turbo --code_model_name Qwen/Qwen3-8B --dataset_path datasets_examples/tat.jsonl --task tat

# ========================================
# Supported Model Patterns
# ========================================
# GPT Models (routed to OpenAI):
# - gpt-3.5-turbo, gpt-35-turbo
# - gpt-4, gpt-4-turbo, gpt-4o

# Open-source Models (routed to RunPod vLLM):
# - Models containing: qwen, llama, mistral, phi, codellama
# - Examples: Qwen/Qwen3-8B, meta-llama/Llama-3-8B, mistralai/Mistral-7B

# ========================================
# Migration Notes
# ========================================
# This configuration replaces the previous Azure OpenAI setup.
# All LLM calls now use the standard OpenAI library format.
# Model routing is automatic based on model name patterns.